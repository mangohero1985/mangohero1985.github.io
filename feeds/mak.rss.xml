<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>MACHINE LEARING NOTE</title><link>http://mangohero1985.github.io/</link><description></description><atom:link href="http://mangohero1985.github.io//feeds/mak.rss.xml" rel="self"></atom:link><lastBuildDate>Wed, 17 Feb 2016 00:00:00 -0800</lastBuildDate><item><title>how to tackle real life problem by machine learning?</title><link>http://mangohero1985.github.io/machine_learning.html</link><description>&lt;h2&gt;Table of contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Data exploration&lt;ol&gt;
&lt;li&gt;type of data&lt;/li&gt;
&lt;li&gt;descriptive statistics&lt;/li&gt;
&lt;li&gt;visualization&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Data Preprocessing&lt;ol&gt;
&lt;li&gt;missing value treatment&lt;/li&gt;
&lt;li&gt;outlier treatment &lt;/li&gt;
&lt;li&gt;sampling&lt;/li&gt;
&lt;li&gt;dimension reduction&lt;/li&gt;
&lt;li&gt;data normalization&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Feature enginerring&lt;ol&gt;
&lt;li&gt;feature extraction&lt;/li&gt;
&lt;li&gt;data transformation&lt;/li&gt;
&lt;li&gt;feature construction/creation&lt;/li&gt;
&lt;li&gt;feature learning&lt;/li&gt;
&lt;li&gt;feature selection&lt;/li&gt;
&lt;li&gt;feature importance&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Algorithm   &lt;/li&gt;
&lt;li&gt;model selectino &lt;/li&gt;
&lt;li&gt;evaluation&lt;ol&gt;
&lt;li&gt;debug&lt;ul&gt;
&lt;li&gt;learning curve&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Data exlporation&lt;/h2&gt;
&lt;h3&gt;objective&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;helping to select the right tool for preprocessing&lt;/li&gt;
&lt;li&gt;recognize pattern by human&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Exploratory Data Analysis&lt;/h3&gt;
&lt;p&gt;Exploratory Data Analysis (EDA), was introduced by John Tukey in the 1960s
- http://blogs.technet.com/b/machinelearning/archive/2015/09/24/data-exploration-with-azure-ml.aspx&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;correlation between predicted variable and feature&lt;/li&gt;
&lt;li&gt;correlation between features&lt;/li&gt;
&lt;li&gt;outliers since it doesnt reflect the real data&lt;/li&gt;
&lt;li&gt;degenerate features? e.g feature of many 0&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;biases in data?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;type of data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;variable category: continuous, catagorical&lt;/li&gt;
&lt;li&gt;type of variable: predictor, target&lt;/li&gt;
&lt;li&gt;data type 
    *record data, data matrix(numeric only), document data, transaction data, graph data, ordered data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;analysis&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;univariate&lt;ul&gt;
&lt;li&gt;discriptive statistics: central of tendency, dispersion&lt;/li&gt;
&lt;li&gt;frequency table&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;bivariate&lt;ul&gt;
&lt;li&gt;continuous data: correlation chi-square test (test for dependence)&lt;/li&gt;
&lt;li&gt;categorical data: two-way table, chi-square test&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;visualization&lt;ul&gt;
&lt;li&gt;objective&lt;ul&gt;
&lt;li&gt;detect general patterns and trends&lt;/li&gt;
&lt;li&gt;detect outliers and unusual patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;visualization techniques&lt;ul&gt;
&lt;li&gt;bar chart&lt;ul&gt;
&lt;li&gt;count of data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;histogram &lt;ul&gt;
&lt;li&gt;distribution of attribute for modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;factor plot&lt;ul&gt;
&lt;li&gt;to-do&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;box plot&lt;ul&gt;
&lt;li&gt;outlier detection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;scatter plots&lt;ul&gt;
&lt;li&gt;correlation between Y and X&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;matrix plots&lt;ul&gt;
&lt;li&gt;useful when objects are sorted according to class&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;correlation matrix&lt;/li&gt;
&lt;li&gt;parallel coordinates&lt;ul&gt;
&lt;li&gt;visualize high dimensional data&lt;/li&gt;
&lt;li&gt;x-axis is attribute, y-axis is value, each line is an object, use differnt colored line for different class&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;note&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;in a histogram, if the distribution of class in a attribute is uneven, i.e some class happened to occur more, then this attribute is useful since we have higher chance of correct prediction in the higher frequency class&lt;/li&gt;
&lt;li&gt;in factor plot, difference in value if good, since we can differntiate class easily by looking at the value&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Classification Binary Class
to-do: write down what to plot for different cases and what information can be draw for each plot&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;cata.bin&lt;/th&gt;
&lt;th&gt;cata.mult&lt;/th&gt;
&lt;th&gt;cont&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;bar chart&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;bar chart by class&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;factor plot&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;histogram&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pdf&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;box plot&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;factor plot&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scatter plots&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;matrix plots&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;correlation matrix&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;parallel coordinates&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Data pre-processing:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;missing value treatment&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generative model&lt;ul&gt;
&lt;li&gt;intergrate our missing variables&lt;/li&gt;
&lt;li&gt;omit the attribute&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;discriminative&lt;ul&gt;
&lt;li&gt;df.info()&lt;/li&gt;
&lt;li&gt;omit the attribute&lt;/li&gt;
&lt;li&gt;numeric&lt;ul&gt;
&lt;li&gt;"recover" the data by generating sample from empirical distribution, e.g random number from mean - sd to mean + sd&lt;/li&gt;
&lt;li&gt;fill in with mean value&lt;/li&gt;
&lt;li&gt;treat as hidden variables and apply EM &lt;a href="http://mlg.eng.cam.ac.uk/zoubin/papers/nips93.pdf"&gt;paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;catagorical&lt;ul&gt;
&lt;li&gt;encode a missing value&lt;/li&gt;
&lt;li&gt;treat it as mode value?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;outlier &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;sampling&lt;/li&gt;
&lt;li&gt;dimension reduction&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;http://deblivingdata.net/machine-learning-tricks/&lt;ul&gt;
&lt;li&gt;feature correlated with predicted variable&lt;/li&gt;
&lt;li&gt;remove outlier? why?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Feature engineering&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;feature extraction&lt;ul&gt;
&lt;li&gt;remove unnecessary column e.g ID, name,...&lt;/li&gt;
&lt;li&gt;distribution of features overall and in each class (classification problem)&lt;ul&gt;
&lt;li&gt;catagorical: sns.countplot(x=, hu=,), sns.factorplot(give mean and also sd)&lt;/li&gt;
&lt;li&gt;continous: df.hist(bins= ) bins use to convert continuous to catagorial data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;data transformation&lt;ul&gt;
&lt;li&gt;objective:&lt;ul&gt;
&lt;li&gt;change scale e.g non-linear to linear, skewed dist. to symmetric dist&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;logarithm, square, square root, binning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature construction/creation&lt;ul&gt;
&lt;li&gt;converting numerical to categorical&lt;/li&gt;
&lt;li&gt;creating dummy variable for categorical data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature learning&lt;/li&gt;
&lt;li&gt;feature importance&lt;/li&gt;
&lt;li&gt;feature selection&lt;ul&gt;
&lt;li&gt;what? when? why? how?&lt;/li&gt;
&lt;li&gt;why?&lt;ul&gt;
&lt;li&gt;in a small feature set (lower dim), it is easier for data visualization and data understanding&lt;/li&gt;
&lt;li&gt;reducing the measurement and storage requirements&lt;/li&gt;
&lt;li&gt;reducing training and utilization times&lt;/li&gt;
&lt;li&gt;&lt;code&gt;defying the surse of dimensionality to improve prediction performance ?&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how?&lt;ol&gt;
&lt;li&gt;filtering&lt;ul&gt;
&lt;li&gt;select variables by ranking them with score function, the individual predictive power, usually correlation&lt;ul&gt;
&lt;li&gt;e.g in regression: correlation coefficients which is related to R square in linear regression so higher correlation means better fit, f regression (sklean)&lt;ul&gt;
&lt;li&gt;f regression test importance of coefficient(s) against intercept, i.e if the model is predictive or not&lt;/li&gt;
&lt;li&gt;t regression is a univariate case of f regression&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;in classification: score function may be accuracy, false positive rate, AUC, &lt;code&gt;fisher's criterion(Duda et al., 2001)&lt;/code&gt;, chi2 (sklearn)...&lt;ul&gt;
&lt;li&gt;chi2 test tests for dependence between ONE predictor and target&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;in text classification, Forman (2003)&lt;/li&gt;
&lt;li&gt;for both regression and classification: empirical estimates of the mutual information (dependence between distribution of x and y) , for continouous variables, discretizing the variables&lt;/li&gt;
&lt;li&gt;information gain??&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;advantages:&lt;ul&gt;
&lt;li&gt;very fast&lt;/li&gt;
&lt;li&gt;simple to apply&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;disadvantages&lt;ul&gt;
&lt;li&gt;doesnt take into account interactions between features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when to use?&lt;ul&gt;
&lt;li&gt;save running time by using light filtering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;subset selection &lt;ul&gt;
&lt;li&gt;assess subsets of variables according to their usefulness to a given predictor, usefulness is measure in objective function&lt;/li&gt;
&lt;li&gt;three methods: wrappers, filters, embedded&lt;/li&gt;
&lt;li&gt;warppers and &lt;code&gt;embedded&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;forward selection and backward elimination (AIC, BIC)&lt;/li&gt;
&lt;li&gt;independent of algorithm&lt;/li&gt;
&lt;li&gt;optimizing objective function e.g RMS in regression, f-regression for regression&lt;/li&gt;
&lt;li&gt;objective function can be average of CV instead of training error to prevent overfitting, trade-off between model complexity and predictive power,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nested subset method&lt;ul&gt;
&lt;li&gt;&lt;code&gt;??&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;filters for subset selection &lt;ul&gt;
&lt;li&gt;&lt;code&gt;??&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;direct objective optimizatio&lt;ul&gt;
&lt;li&gt;directly optimizing cost function by selection subset of features&lt;/li&gt;
&lt;li&gt;e.g subset selection in SVM&lt;ul&gt;
&lt;li&gt;minimise error function $R^2W^2(\sigma)$ w.r.t $\sigma$ by gradient descend instead of using heuristic method for $\sigma=[0,1]$. $\sigma$ is subjected to constrain that $\sum_i\sigma_i=m$ in 
$$R^2W^2(\sigma)+\lambda\sum_i(\sigma_i)^p$$&lt;/li&gt;
&lt;li&gt;train using linear SVM and shrink its coefficient step by step until  generalization error is converged then eliminate features whose coefficient is close to 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;regularization&lt;ul&gt;
&lt;li&gt;objective: reduce complexity of model hence prevent overfitting i.e training error increase but test error decrease&lt;/li&gt;
&lt;li&gt;when? &lt;ul&gt;
&lt;li&gt;when we have lots of features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how?&lt;ul&gt;
&lt;li&gt;sacrificing little bias and in turn reduce variance, hence the model is more suitable for new data though overall error increase&lt;/li&gt;
&lt;li&gt;reducing some parameters to zero as lambda increase&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;how to solve lambda and n? also geometric interpretation&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;when to use?&lt;ul&gt;
&lt;li&gt;save computational cost --&amp;gt; dimension reduction&lt;/li&gt;
&lt;li&gt;&lt;code&gt;when you care about the feature themselves?&lt;/code&gt; [Guyon et al., Causal feature selection, 07]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when not to use&lt;ul&gt;
&lt;li&gt;email classification case, long tail word may be useful, zipf law&lt;/li&gt;
&lt;li&gt;if computational cost doesnt matter, we can try regularization instead&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;note:
1.  generalization error also calls out-of-sample error is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.
2. &lt;/p&gt;
&lt;h3&gt;reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Course Lecture Slides and Materials, Columbia
    http://www.columbia.edu/~rsb2162/FES2013/materials.html&lt;/li&gt;
&lt;li&gt;JMLR Special Issue on Variable and Feature Selection
    http://jmlr.org/papers/special/feature03.html&lt;/li&gt;
&lt;li&gt;feature extraction foundations and applications
    http://www.springer.com/jp/book/9783540354871&lt;/li&gt;
&lt;li&gt;explanation about feature engineering
    http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/&lt;/li&gt;
&lt;li&gt;kaggle competition -- predicting people who will go to hospital:&lt;ul&gt;
&lt;li&gt;sql code for creating features&lt;/li&gt;
&lt;li&gt;list of features&lt;/li&gt;
&lt;li&gt;https://kaggle2.blob.core.windows.net/wiki-files/327/e4cd1d25-eca9-49ca-9593-b254a773fe03/Market%20Makers%20-%20Milestone%201%20Description%20V2%201.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature selection by alex bouchard a berkeley lecture&lt;ul&gt;
&lt;li&gt;http://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;an introduction to variable and feature selectoin &lt;ul&gt;
&lt;li&gt;http://www.ai.mit.edu/projects/jmlr/papers/volume3/guyon03a/source/old/guyon03a.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature selection for svm&lt;ul&gt;
&lt;li&gt;http://papers.nips.cc/paper/1850-feature-selection-for-svms.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;micelleneous&lt;/h2&gt;
&lt;p&gt;remove duplicated value
continuous: normalizatoin
use excel or google refine for prototyping
for continuous variables and classificatoin: distribution over different class?&lt;/p&gt;
&lt;h2&gt;reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://cs229.stanford.edu/materials/ML-advice.pdf"&gt;advice for applying machine learning by andrew ng&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mak</dc:creator><pubDate>Wed, 17 Feb 2016 00:00:00 -0800</pubDate><guid>tag:mangohero1985.github.io,2016-02-17:machine_learning.html</guid><category>Machine learning</category></item></channel></rss>