<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>MACHINE LEARING NOTE</title><link href="http://mangohero1985.github.io/" rel="alternate"></link><link href="http://mangohero1985.github.io//feeds/deep-learning.atom.xml" rel="self"></link><id>http://mangohero1985.github.io/</id><updated>2016-02-17T00:00:00-08:00</updated><entry><title>Negtive sampling</title><link href="http://mangohero1985.github.io/negtive_sampling.html" rel="alternate"></link><updated>2016-02-17T00:00:00-08:00</updated><author><name>Mango</name></author><id>tag:mangohero1985.github.io,2016-02-17:negtive_sampling.html</id><summary type="html">&lt;p&gt;The idea of word2vec is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. In equation (3) of the paper you link to, ignore the exponentiation for a moment. You have&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;  v_c * v_w/ sum(v_c1 * v_w)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The numerator is basically the similarity between words c (the context) and w (the target) word. The denominator computes the similarity of all other contexts c1 and the target word w. Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. However, computing this can be very slow, because there are many contexts c1. Negative sampling is one of the ways of addressing this problem- just select a couple of contexts c1 at random. The end result is that if cat appears in the context of food, then the vector of food is more similar to the vector of cat (as measures by their dot product) than the vectors of several other randomly chosen words (e.g. democracy, greed, Freddy), instead of all other words in language. This makes word2vec much much faster to train&lt;sup id="fnref:github"&gt;&lt;a class="footnote-ref" href="#fn:github" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:github"&gt;
&lt;p&gt;&lt;a href="http://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term/27864657#27864657"&gt;github&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:github" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="Negtive sampling is a optimized sampling method which can be combined with softmax to achieve higher efficiency on computation."></category></entry><entry><title>Continuous bag of words</title><link href="http://mangohero1985.github.io/cbow.html" rel="alternate"></link><updated>2016-02-17T00:00:00-08:00</updated><author><name>Mango</name></author><id>tag:mangohero1985.github.io,2016-02-17:cbow.html</id><summary type="html">&lt;h1&gt;Cbow&lt;/h1&gt;
&lt;p&gt;The input to the model could be  wi−2,wi−1,wi+1,wi+2wi−2,wi−1,wi+1,wi+2 , the preceding and following words of the current word we are at. The output of the neural network will be  wi. Hence you can think of the task as "predicting the word given its context"
Note that the number of words we use depends on your setting for the window size&lt;sup id="fnref:cbow"&gt;&lt;a class="footnote-ref" href="#fn:cbow" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Compared with Skip-gram, Cbow several times faster to train than skip-gram and slight better accuracy on frequent word. Skip-gram have better performance than Cbow on small corpus.&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:cbow"&gt;
&lt;p&gt;&lt;a href="https://www.quora.com/What-are-the-continuous-bag-of-words-and-skip-gram-architectures-in-laymans-terms"&gt;Quroa&lt;/a&gt;&amp;#160;&lt;a class="footnote-backref" href="#fnref:cbow" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="Neural language model"></category><category term="Deep learning"></category></entry><entry><title>Skip-gram</title><link href="http://mangohero1985.github.io/skip_gram.html" rel="alternate"></link><updated>2016-02-16T00:00:00-08:00</updated><author><name>Mango</name></author><id>tag:mangohero1985.github.io,2016-02-17:skip_gram.html</id><summary type="html">&lt;h2&gt;Skip-gram sampling&lt;/h2&gt;
&lt;p&gt;Skip-grams is a technique where by n-grams are still stored  to model language, but they allow for tokens to be skipped. This method can overcome the data sparsity problem to some extend&lt;sup id="fnref:Explaination"&gt;&lt;a class="footnote-ref" href="#fn:Explaination" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;. &lt;/p&gt;
&lt;p&gt;Skip-gram can sample the context in the way of "jumping". For example, set a k of n-gram skip-gram model. k means the range of word-skip and n is still the concept of n-gram. Give out a sentence of w1 to w10. If we set the current word is w5. In this case, we use a model of 3 of 2 skip-gram. Then, the context would be extract as 2-gram and the word-skip be 4. the prefix context is {w2,w3,w4} and the surfix context is {w6,w7,w8}. The model would sample 2 words from context either. Sampling result would be {wx,wy|x,y from (w2,w3,w4,w6,w7,w8)}&lt;/p&gt;
&lt;p&gt;Example from the paper&lt;sup id="fnref:Explaination"&gt;&lt;a class="footnote-ref" href="#fn:Explaination" rel="footnote"&gt;1&lt;/a&gt;&lt;/sup&gt;
&lt;img alt="skip_gram" src="/images/skip_gram.png" /&gt;&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id="fn:Explaination"&gt;
&lt;p&gt;Paper: A Closer Look at Skip-gram Modelling&amp;#160;&lt;a class="footnote-backref" href="#fnref:Explaination" rev="footnote" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><category term="Neural language model"></category><category term="Deep learning"></category></entry></feed>