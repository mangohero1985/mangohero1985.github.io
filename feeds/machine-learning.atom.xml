<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>MACHINE LEARING NOTE</title><link href="http://mangohero1985.github.io/" rel="alternate"></link><link href="http://mangohero1985.github.io//feeds/machine-learning.atom.xml" rel="self"></link><id>http://mangohero1985.github.io/</id><updated>2016-03-23T00:00:00-07:00</updated><entry><title>Inference and decision</title><link href="http://mangohero1985.github.io/Inference_and_decision.html" rel="alternate"></link><updated>2016-03-23T00:00:00-07:00</updated><author><name>Mango</name></author><id>tag:mangohero1985.github.io,2016-03-24:Inference_and_decision.html</id><summary type="html">&lt;p&gt;In machine learning, we can identify three distinct approaches to solving decison problems. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generative model: First solve the inference problem of determining the conditional densities p(x|C) for each class C individually. Also separately get the prior p(C). Use these two probabilities to find posterior in form of Baysian theorem: &lt;img alt="Baysian theorem" src="/images/Baysian_theorem.png" /&gt;
&lt;img alt="denominator" src="/images/denominator.png" /&gt;
we can also model the joint distribution p(C,x) to obtain the posterior probabilites. Having found the posterior probabilities, then use decision theory to determine the class for x. This model is known as generative models, because by sampling from them it is possible to generate synthetic data points in the input space.&lt;/li&gt;
&lt;li&gt;First solve the inference problem of determining the posterior p(C|x), and then subsequently use decision theory to assign each new x to one of the classes. Approaches that model the posterior directly are called discriminative models.&lt;/li&gt;
&lt;li&gt;Find a function f(x), called a discriminant function which can seperate x into class 0 or class 1. In this case, probabilites play no role.&lt;/li&gt;
&lt;/ol&gt;</summary><category term="Machine learning"></category></entry><entry><title>How to tackle real life problem by machine learning?</title><link href="http://mangohero1985.github.io/machine_learning.html" rel="alternate"></link><updated>2016-02-17T00:00:00-08:00</updated><author><name>Mak</name></author><id>tag:mangohero1985.github.io,2016-03-24:machine_learning.html</id><summary type="html">&lt;h2&gt;Table of contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Data exploration&lt;ol&gt;
&lt;li&gt;type of data&lt;/li&gt;
&lt;li&gt;descriptive statistics&lt;/li&gt;
&lt;li&gt;visualization&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Data Preprocessing&lt;ol&gt;
&lt;li&gt;missing value treatment&lt;/li&gt;
&lt;li&gt;outlier treatment &lt;/li&gt;
&lt;li&gt;sampling&lt;/li&gt;
&lt;li&gt;dimension reduction&lt;/li&gt;
&lt;li&gt;data normalization&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Feature enginerring&lt;ol&gt;
&lt;li&gt;feature extraction&lt;/li&gt;
&lt;li&gt;data transformation&lt;/li&gt;
&lt;li&gt;feature construction/creation&lt;/li&gt;
&lt;li&gt;feature learning&lt;/li&gt;
&lt;li&gt;feature selection&lt;/li&gt;
&lt;li&gt;feature importance&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Algorithm   &lt;/li&gt;
&lt;li&gt;model selectino &lt;/li&gt;
&lt;li&gt;evaluation&lt;ol&gt;
&lt;li&gt;debug&lt;ul&gt;
&lt;li&gt;learning curve&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error analysis&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Data exlporation&lt;/h2&gt;
&lt;h3&gt;objective&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;helping to select the right tool for preprocessing&lt;/li&gt;
&lt;li&gt;recognize pattern by human&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Exploratory Data Analysis&lt;/h3&gt;
&lt;p&gt;Exploratory Data Analysis (EDA), was introduced by John Tukey in the 1960s
- http://blogs.technet.com/b/machinelearning/archive/2015/09/24/data-exploration-with-azure-ml.aspx&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;correlation between predicted variable and feature&lt;/li&gt;
&lt;li&gt;correlation between features&lt;/li&gt;
&lt;li&gt;outliers since it doesnt reflect the real data&lt;/li&gt;
&lt;li&gt;degenerate features? e.g feature of many 0&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;biases in data?&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;type of data&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;variable category: continuous, catagorical&lt;/li&gt;
&lt;li&gt;type of variable: predictor, target&lt;/li&gt;
&lt;li&gt;data type 
    *record data, data matrix(numeric only), document data, transaction data, graph data, ordered data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;analysis&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;univariate&lt;ul&gt;
&lt;li&gt;discriptive statistics: central of tendency, dispersion&lt;/li&gt;
&lt;li&gt;frequency table&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;bivariate&lt;ul&gt;
&lt;li&gt;continuous data: correlation chi-square test (test for dependence)&lt;/li&gt;
&lt;li&gt;categorical data: two-way table, chi-square test&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;visualization&lt;ul&gt;
&lt;li&gt;objective&lt;ul&gt;
&lt;li&gt;detect general patterns and trends&lt;/li&gt;
&lt;li&gt;detect outliers and unusual patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;visualization techniques&lt;ul&gt;
&lt;li&gt;bar chart&lt;ul&gt;
&lt;li&gt;count of data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;histogram &lt;ul&gt;
&lt;li&gt;distribution of attribute for modeling&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;factor plot&lt;ul&gt;
&lt;li&gt;to-do&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;box plot&lt;ul&gt;
&lt;li&gt;outlier detection&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;scatter plots&lt;ul&gt;
&lt;li&gt;correlation between Y and X&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;matrix plots&lt;ul&gt;
&lt;li&gt;useful when objects are sorted according to class&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;correlation matrix&lt;/li&gt;
&lt;li&gt;parallel coordinates&lt;ul&gt;
&lt;li&gt;visualize high dimensional data&lt;/li&gt;
&lt;li&gt;x-axis is attribute, y-axis is value, each line is an object, use differnt colored line for different class&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;note&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;in a histogram, if the distribution of class in a attribute is uneven, i.e some class happened to occur more, then this attribute is useful since we have higher chance of correct prediction in the higher frequency class&lt;/li&gt;
&lt;li&gt;in factor plot, difference in value if good, since we can differntiate class easily by looking at the value&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Classification Binary Class
to-do: write down what to plot for different cases and what information can be draw for each plot&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;cata.bin&lt;/th&gt;
&lt;th&gt;cata.mult&lt;/th&gt;
&lt;th&gt;cont&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;bar chart&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;bar chart by class&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;factor plot&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;histogram&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pdf&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;Y&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;box plot&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;factor plot&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;scatter plots&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;matrix plots&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;correlation matrix&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;parallel coordinates&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2&gt;Data pre-processing:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;missing value treatment&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generative model&lt;ul&gt;
&lt;li&gt;intergrate our missing variables&lt;/li&gt;
&lt;li&gt;omit the attribute&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;discriminative&lt;ul&gt;
&lt;li&gt;df.info()&lt;/li&gt;
&lt;li&gt;omit the attribute&lt;/li&gt;
&lt;li&gt;numeric&lt;ul&gt;
&lt;li&gt;"recover" the data by generating sample from empirical distribution, e.g random number from mean - sd to mean + sd&lt;/li&gt;
&lt;li&gt;fill in with mean value&lt;/li&gt;
&lt;li&gt;treat as hidden variables and apply EM &lt;a href="http://mlg.eng.cam.ac.uk/zoubin/papers/nips93.pdf"&gt;paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;catagorical&lt;ul&gt;
&lt;li&gt;encode a missing value&lt;/li&gt;
&lt;li&gt;treat it as mode value?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;outlier &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;sampling&lt;/li&gt;
&lt;li&gt;dimension reduction&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;http://deblivingdata.net/machine-learning-tricks/&lt;ul&gt;
&lt;li&gt;feature correlated with predicted variable&lt;/li&gt;
&lt;li&gt;remove outlier? why?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Feature engineering&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;feature extraction&lt;ul&gt;
&lt;li&gt;remove unnecessary column e.g ID, name,...&lt;/li&gt;
&lt;li&gt;distribution of features overall and in each class (classification problem)&lt;ul&gt;
&lt;li&gt;catagorical: sns.countplot(x=, hu=,), sns.factorplot(give mean and also sd)&lt;/li&gt;
&lt;li&gt;continous: df.hist(bins= ) bins use to convert continuous to catagorial data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;data transformation&lt;ul&gt;
&lt;li&gt;objective:&lt;ul&gt;
&lt;li&gt;change scale e.g non-linear to linear, skewed dist. to symmetric dist&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;logarithm, square, square root, binning&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature construction/creation&lt;ul&gt;
&lt;li&gt;converting numerical to categorical&lt;/li&gt;
&lt;li&gt;creating dummy variable for categorical data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature learning&lt;/li&gt;
&lt;li&gt;feature importance&lt;/li&gt;
&lt;li&gt;feature selection&lt;ul&gt;
&lt;li&gt;what? when? why? how?&lt;/li&gt;
&lt;li&gt;why?&lt;ul&gt;
&lt;li&gt;in a small feature set (lower dim), it is easier for data visualization and data understanding&lt;/li&gt;
&lt;li&gt;reducing the measurement and storage requirements&lt;/li&gt;
&lt;li&gt;reducing training and utilization times&lt;/li&gt;
&lt;li&gt;&lt;code&gt;defying the surse of dimensionality to improve prediction performance ?&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how?&lt;ol&gt;
&lt;li&gt;filtering&lt;ul&gt;
&lt;li&gt;select variables by ranking them with score function, the individual predictive power, usually correlation&lt;ul&gt;
&lt;li&gt;e.g in regression: correlation coefficients which is related to R square in linear regression so higher correlation means better fit, f regression (sklean)&lt;ul&gt;
&lt;li&gt;f regression test importance of coefficient(s) against intercept, i.e if the model is predictive or not&lt;/li&gt;
&lt;li&gt;t regression is a univariate case of f regression&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;in classification: score function may be accuracy, false positive rate, AUC, &lt;code&gt;fisher's criterion(Duda et al., 2001)&lt;/code&gt;, chi2 (sklearn)...&lt;ul&gt;
&lt;li&gt;chi2 test tests for dependence between ONE predictor and target&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;in text classification, Forman (2003)&lt;/li&gt;
&lt;li&gt;for both regression and classification: empirical estimates of the mutual information (dependence between distribution of x and y) , for continouous variables, discretizing the variables&lt;/li&gt;
&lt;li&gt;information gain??&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;advantages:&lt;ul&gt;
&lt;li&gt;very fast&lt;/li&gt;
&lt;li&gt;simple to apply&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;disadvantages&lt;ul&gt;
&lt;li&gt;doesnt take into account interactions between features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when to use?&lt;ul&gt;
&lt;li&gt;save running time by using light filtering&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;subset selection &lt;ul&gt;
&lt;li&gt;assess subsets of variables according to their usefulness to a given predictor, usefulness is measure in objective function&lt;/li&gt;
&lt;li&gt;three methods: wrappers, filters, embedded&lt;/li&gt;
&lt;li&gt;warppers and &lt;code&gt;embedded&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;forward selection and backward elimination (AIC, BIC)&lt;/li&gt;
&lt;li&gt;independent of algorithm&lt;/li&gt;
&lt;li&gt;optimizing objective function e.g RMS in regression, f-regression for regression&lt;/li&gt;
&lt;li&gt;objective function can be average of CV instead of training error to prevent overfitting, trade-off between model complexity and predictive power,&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;nested subset method&lt;ul&gt;
&lt;li&gt;&lt;code&gt;??&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;filters for subset selection &lt;ul&gt;
&lt;li&gt;&lt;code&gt;??&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;direct objective optimizatio&lt;ul&gt;
&lt;li&gt;directly optimizing cost function by selection subset of features&lt;/li&gt;
&lt;li&gt;e.g subset selection in SVM&lt;ul&gt;
&lt;li&gt;minimise error function $R^2W^2(\sigma)$ w.r.t $\sigma$ by gradient descend instead of using heuristic method for $\sigma=[0,1]$. $\sigma$ is subjected to constrain that $\sum_i\sigma_i=m$ in 
$$R^2W^2(\sigma)+\lambda\sum_i(\sigma_i)^p$$&lt;/li&gt;
&lt;li&gt;train using linear SVM and shrink its coefficient step by step until  generalization error is converged then eliminate features whose coefficient is close to 0&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;regularization&lt;ul&gt;
&lt;li&gt;objective: reduce complexity of model hence prevent overfitting i.e training error increase but test error decrease&lt;/li&gt;
&lt;li&gt;when? &lt;ul&gt;
&lt;li&gt;when we have lots of features&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;how?&lt;ul&gt;
&lt;li&gt;sacrificing little bias and in turn reduce variance, hence the model is more suitable for new data though overall error increase&lt;/li&gt;
&lt;li&gt;reducing some parameters to zero as lambda increase&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;how to solve lambda and n? also geometric interpretation&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;when to use?&lt;ul&gt;
&lt;li&gt;save computational cost --&amp;gt; dimension reduction&lt;/li&gt;
&lt;li&gt;&lt;code&gt;when you care about the feature themselves?&lt;/code&gt; [Guyon et al., Causal feature selection, 07]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;when not to use&lt;ul&gt;
&lt;li&gt;email classification case, long tail word may be useful, zipf law&lt;/li&gt;
&lt;li&gt;if computational cost doesnt matter, we can try regularization instead&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;note:
1.  generalization error also calls out-of-sample error is a measure of how accurately an algorithm is able to predict outcome values for previously unseen data.
2. &lt;/p&gt;
&lt;h3&gt;reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Course Lecture Slides and Materials, Columbia
    http://www.columbia.edu/~rsb2162/FES2013/materials.html&lt;/li&gt;
&lt;li&gt;JMLR Special Issue on Variable and Feature Selection
    http://jmlr.org/papers/special/feature03.html&lt;/li&gt;
&lt;li&gt;feature extraction foundations and applications
    http://www.springer.com/jp/book/9783540354871&lt;/li&gt;
&lt;li&gt;explanation about feature engineering
    http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/&lt;/li&gt;
&lt;li&gt;kaggle competition -- predicting people who will go to hospital:&lt;ul&gt;
&lt;li&gt;sql code for creating features&lt;/li&gt;
&lt;li&gt;list of features&lt;/li&gt;
&lt;li&gt;https://kaggle2.blob.core.windows.net/wiki-files/327/e4cd1d25-eca9-49ca-9593-b254a773fe03/Market%20Makers%20-%20Milestone%201%20Description%20V2%201.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature selection by alex bouchard a berkeley lecture&lt;ul&gt;
&lt;li&gt;http://www.cs.berkeley.edu/~jordan/courses/294-fall09/lectures/feature/&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;an introduction to variable and feature selectoin &lt;ul&gt;
&lt;li&gt;http://www.ai.mit.edu/projects/jmlr/papers/volume3/guyon03a/source/old/guyon03a.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;feature selection for svm&lt;ul&gt;
&lt;li&gt;http://papers.nips.cc/paper/1850-feature-selection-for-svms.pdf&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;micelleneous&lt;/h2&gt;
&lt;p&gt;remove duplicated value
continuous: normalizatoin
use excel or google refine for prototyping
for continuous variables and classificatoin: distribution over different class?&lt;/p&gt;
&lt;h2&gt;reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://cs229.stanford.edu/materials/ML-advice.pdf"&gt;advice for applying machine learning by andrew ng&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;script type= "text/javascript"&gt;
    if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
        var mathjaxscript = document.createElement('script');
        mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
        mathjaxscript.type = 'text/javascript';
        mathjaxscript.src = 'https:' == document.location.protocol
                ? 'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'
                : 'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
        mathjaxscript[(window.opera ? "innerHTML" : "text")] =
            "MathJax.Hub.Config({" +
            "    config: ['MMLorHTML.js']," +
            "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
            "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
            "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
            "    displayAlign: 'center'," +
            "    displayIndent: '0em'," +
            "    showMathMenu: true," +
            "    tex2jax: { " +
            "        inlineMath: [ ['$','$'] ], " +
            "        displayMath: [ ['$$','$$'] ]," +
            "        processEscapes: true," +
            "        preview: 'TeX'," +
            "    }, " +
            "    'HTML-CSS': { " +
            "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
            "    } " +
            "}); ";
        (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
    }
&lt;/script&gt;
</summary><category term="Machine learning"></category></entry><entry><title>Adaptive Boosting</title><link href="http://mangohero1985.github.io/Adaptive_boosting.html" rel="alternate"></link><updated>2015-12-24T00:00:00-08:00</updated><author><name>Mango</name></author><id>tag:mangohero1985.github.io,2015-12-24:Adaptive_boosting.html</id><summary type="html">&lt;h2&gt;Description&lt;/h2&gt;
&lt;p&gt;AdaBoost is a powerful classifier that work well on both basic and more complex recogition problems. AdaBoost works by creating a highly accurate classifier by combining many relatively weak and inaccurate classifiers.&lt;/p&gt;</summary><category term="Aggregation"></category></entry><entry><title>Distance Metrics</title><link href="http://mangohero1985.github.io/distance_metics.html" rel="alternate"></link><updated>2015-12-15T00:00:00-08:00</updated><author><name>Mango</name></author><id>tag:mangohero1985.github.io,2015-12-11:distance_metics.html</id><summary type="html">&lt;h2&gt;Metrics Space&lt;/h2&gt;
&lt;p&gt;Given a set &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt; is a metirc space if it keeps to a function &lt;strong&gt;&lt;em&gt;d(x,y)&lt;/em&gt;&lt;/strong&gt; that can measure the distance between any two points &lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;em&gt;y&lt;/em&gt;&lt;/strong&gt; in space of &lt;strong&gt;&lt;em&gt;X&lt;/em&gt;&lt;/strong&gt;. Then, &lt;strong&gt;&lt;em&gt;d&lt;/em&gt;&lt;/strong&gt; must satisfy the axioms of the metrics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Non-negativity: &lt;strong&gt;&lt;em&gt;d(x,y)&amp;gt;=0&lt;/em&gt;&lt;/strong&gt;, &lt;strong&gt;&lt;em&gt;d(x,y)=0&lt;/em&gt;&lt;/strong&gt; if and only if &lt;strong&gt;&lt;em&gt;x=y&lt;/em&gt;&lt;/strong&gt; &lt;/li&gt;
&lt;li&gt;Symmetry: &lt;strong&gt;&lt;em&gt;d(x,y)=d(y,x)&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Triangle inequality: &lt;strong&gt;&lt;em&gt;d(x,y)+d(y,z) &amp;gt;= d(x,z)&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;&lt;a name="Minkowski_distance"&gt;&lt;/a&gt;Minkowski distance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Minkowski dishtance&lt;/strong&gt; is the most common distance measure method, there are two points &lt;strong&gt;&lt;em&gt;x&lt;/em&gt;&lt;/strong&gt; , &lt;strong&gt;&lt;em&gt;y&lt;/em&gt;&lt;/strong&gt; as follows:
&lt;img alt="Minkowski1" src="/images/Minkowski_distance_1.png" /&gt;
&lt;img alt="Minkowski2" src="/images/Minkowski_distance_2.png" /&gt;&lt;/p&gt;
&lt;p&gt;Here, if the &lt;strong&gt;&lt;em&gt;P&lt;/em&gt;&lt;/strong&gt; is 2, Minkowski distance would be &lt;a href="#Euclidean_distance"&gt;Euclidean distance&lt;/a&gt;. If the &lt;strong&gt;&lt;em&gt;P&lt;/em&gt;&lt;/strong&gt;  is 1, it would be Manhanttan distance.&lt;/p&gt;
&lt;h3&gt;&lt;a name="Euclidean_distance"&gt;&lt;/a&gt;Euclidean distance&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Euclidean distance&lt;/strong&gt; is the "ordianary" disctance between two points in Euclidean space. The points in Euclidean space (N-Space) are called Euclidean vectors. The Euclidean distance of vector of vector &lt;strong&gt;&lt;em&gt;P&lt;/em&gt;&lt;/strong&gt; and vector &lt;strong&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/strong&gt; can be computed as follows:
&lt;img alt="Euclidean" src="/images/Euclidean_distance.png" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a name="Manhattan_distance"&gt;&lt;/a&gt;Manhattan distance(Texicab geometry)&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Manhattan distance&lt;/strong&gt; is a form of geometry in which the usual distance function of metric or Euclidean geomery us replaces by a new metirc in which the distance between two points is the sum of the absolute differences of their &lt;a href="https://en.wikipedia.org/wiki/Cartesian_coordinate_system"&gt;Carteisan coordinates&lt;/a&gt;. Manhatten distance is also known as rectilinear distance L1 distance.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Manhattan_dis" src="/images/Manhattan_dis.png" /&gt;&lt;/p&gt;
&lt;h3&gt;&lt;a name="Chebyshev_distance"&gt;&lt;/a&gt;Chebyshev distance&lt;/h3&gt;
&lt;p&gt;If &lt;strong&gt;&lt;em&gt;P&lt;/em&gt;&lt;/strong&gt; tends to be infinite, &lt;strong&gt;Minkowski distance&lt;/strong&gt; would be &lt;strong&gt;Chebyshev distance&lt;/strong&gt;. It can be expressed as: 
&lt;img alt="chevi_dis" src="/images/Chebyshev_distance.png" /&gt;&lt;/p&gt;
&lt;h3&gt;Extends for Minkowski distance&lt;/h3&gt;
&lt;p&gt;In plain, if the Euclidean distance is set as 1, when &lt;strong&gt;&lt;em&gt;p&lt;/em&gt;&lt;/strong&gt; is 2, the figure would be a round. Thus, what the figures would be when &lt;strong&gt;&lt;em&gt;p&lt;/em&gt;&lt;/strong&gt; is the other value?
&lt;img alt="p_diff" src="/images/p_diff.png" /&gt;&lt;/p&gt;
&lt;p&gt;Commonly, before we are going to use &lt;strong&gt;Minkowski distance&lt;/strong&gt;, a regularion method named &lt;strong&gt;"z-transform"&lt;/strong&gt; is adopted:
&lt;img alt="z-trans" src="/images/z-trans.png" /&gt;
mu: the mean in that dimention&lt;br /&gt;
sigma: the variance in that dimention&lt;/p&gt;
&lt;p&gt;=======&lt;/p&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;a285ab651fa2f537e0c1a7a00d97b05b84db7b8b&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Euclidean Distance&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Euclidean distance&lt;/strong&gt; is the "ordianary" disctance between two points in Euclidean space. The points in Euclidean space (N-Space) are called Euclidean vectors. The Euclidean distance of vector of vector &lt;strong&gt;&lt;em&gt;P&lt;/em&gt;&lt;/strong&gt; and vector &lt;strong&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/strong&gt; can be computed as follows:
&lt;img alt="Euclidean" src="/images/Euclidean_distance.png" /&gt;&lt;/p&gt;</summary><category term="Unsupervised"></category><category term="Clustering"></category></entry></feed>